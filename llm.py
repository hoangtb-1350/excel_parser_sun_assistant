from __future__ import annotations
from abc import ABC

from openai import AzureOpenAI
from pydantic import BaseModel
from pydantic import ConfigDict
import os
import json

try:
    from google.colab import userdata
    GPT4O_KEY = userdata.get('GPT4O_KEY')
    GPT4O_ENDPOINT = userdata.get('GPT4O__ENDPOINT')
    GPT4O_API_VERSION = userdata.get('GPT4O__API_VERSION')
except:
    from dotenv import load_dotenv
    load_dotenv()
    GPT4O_KEY = os.getenv('GRAPHRAG_API_KEY')
    GPT4O_ENDPOINT = os.getenv('GRAPHRAG_API_BASE')
    GPT4O_API_VERSION = os.getenv('GRAPHRAG_API_VERSION')

SYSTEM_MESSAGE = """
# 1. Instruction
You are an expert in analyzing table content.
Given a list positions and contents represent table cells, your mission is converting the input data table into a comprehensive paragraph.
"""

class BaseLLM(BaseModel, ABC):
    """
    Base class for all modules that use a language model.

    Attributes:
    -----------
    model_config : ConfigDict
        Configuration dictionary for the language model.
    llm_settings : AzureOpenAISetting
        Settings for the language model.
    """

    model_config = ConfigDict(extra="allow")

    @property
    def chat_model(self) -> AzureOpenAI:
        return AzureOpenAI(
            api_key=GPT4O_KEY,
            azure_endpoint=GPT4O_ENDPOINT,
            api_version=GPT4O_API_VERSION,
            azure_deployment='GPT4o'
        )

class AnswerGenerator(BaseLLM):
    def generate(
        self,
        system_prompt, user_prompt
    ) -> dict:
        """
        Combines the source content and user question to generate an answer using the Azure OpenAI model.

        Args:
            source_content (str): A string representing the source content.
            user_question (str): A string representing the user's question.
            prompt_type (str): A string indicating which prompt type to use, can be "row" or "cell".

        Returns:
            dict: the answer generated by the Azure OpenAI model
        """

        try:
            result = self.chat_model.chat.completions.create(
                model='gpt-4o',
                seed=0,
                temperature=0,
                max_tokens=4096,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
            )
            result_content = result.choices[0].message.content
            print('gpt ans:', result_content)
            return result_content

        except Exception as e:
            print(f"Some errors when generate answer with GPT: {e}")
            return {}

    def __json_parse(self, text):
        import json
        import re
        match = re.search(r"\{.*\}", text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL)
        if match:
            json_str = match.group()
            try:
                json_object = json.loads(json_str, strict=False)
            except json.decoder.JSONDecodeError:
                json_object = {}
        else:
            json_object = {}

        return json_object
    